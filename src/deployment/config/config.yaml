model:
  base_model: /workspace/models/hf_export
  lora_adapter: /workspace/lora_adapters/megatron_gpt_345m_tuned/megatron_gpt_peft_adapter_tuning.nemo
  max_batch_size: 8
  max_sequence_length: 512
  tensor_parallel_size: 1
  dtype: "bfloat16"

# Export configuration
export:
  dtype: bfloat16
  max_input_len: 256
  max_output_len: 256
  max_batch_size: 8
  tensor_parallelism: 1
  use_lora_plugin: true
  max_lora_rank: 8

server:
  port: 8000
  host: "0.0.0.0"
  max_concurrent_requests: 100
