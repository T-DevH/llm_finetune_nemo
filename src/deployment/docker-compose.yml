version: "3.9"

services:
  llm-inference:
    build:
      context: .
    ports:
      - "8000:8000"
    volumes:
      - ../../results:/app/model
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]